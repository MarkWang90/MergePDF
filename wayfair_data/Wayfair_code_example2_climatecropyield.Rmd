---
title: "R Spatial Panel Data Modeling with Application to Corn Yield"
author: "Mark Wang"
date: "May 2, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Agricultural production is directly influenced by climate and thus is vulnerable to climate change. The research question of this example is to assess the impact of climate change (e.g. temperature and precipitation) on agricultural productivity. Corn production in Corn Belt is used for a case study as it is the most import grain crop in US. 

This code example is inspired by the work of [Schlenker et al. (2009)](http://www.pnas.org/content/106/37/15594), which is originally conducted using STATA, with two extensions:
1. New dataset update to 2015 was collected
2. Schlenker et al. (2009) used the panel model estimation. In reality, crop production shows spatial correlations in natural due to similar soil charastersitics and weather conditions. Thus, a Spatial Panel model is also included and compared.

## Import libraries

```{r echo=FALSE, message=FALSE}
#### 0. install library ####

library(plm)
library(spam)
library(splm)

library(spdep)
library(shapefiles)
library(rgdal)
print("Import library finished")
```

## Define functions
Three functions are defined which will be used repeated in this example:
1. **RMSE** calculates the mean squared error
2. **MakeGraphs** generates side-by-side maps automately
3. **combine.data.shapefile2** combines data with shapefiles, adopted from R package [CARBayes](https://www.rdocumentation.org/packages/CARBayes/versions/5.0) and modified to fit this project.

```{r echo=FALSE}
options(scipen=999)
options(digits=4)

#### 1. pre-defined functions ####


# Function to calculate mean squared error
RMSE <- function(vector1, vector2){  
  return(sqrt(sum((vector1-vector2)^2)/length(vector1)))
}

# Function to make side-by-side maps
# dataf a dataframe with 1st col fips, 2nd col year,, 3rd col value
# samescale equals to TRUE or FALSE, setting graphs of different years in the same scale  
MakeGraphs <- function(dataf,samescale){
  if(samescale){
    yrange <- range(dataf[,3])[2]-range(dataf[,3])[1]
    ylabel <- seq(range(dataf[,3])[1], range(dataf[,3])[2], by=yrange/9)
    colorkey <- list(labels=list(at = ylabel,labels = round(ylabel,1),cex=1))
  } else {colorkey=FALSE}
    
  len_t = length(unique(dataf$year))
  for (t in 1:len_t){
    
    curdata <- dataf[which(dataf$year == unique(dataf$year)[t]),]
    rownames(curdata) <- curdata$fips
    curdata.comb <- combine.data.shapefile2(curdata,shp = shp_c, dbf = dbf_c)
    plot = spplot(curdata.comb,
                  main=paste(colnames(dataf)[3], ":",
                             unique(dataf$year)[t]-10, "-",
                             unique(dataf$year)[t],
                             sep = " "), 
                  at=ylabel,colorkey=colorkey,zcol=3)
    
    position = c((t-1)/len_t,0,t/len_t,1)
    if(t==len_t){
      more=FALSE} else{more=TRUE}
    
    print(plot, position = position, more = more)
  }
}

### Function to combine dataframe with GIS shapefiles
combine.data.shapefile2=function (data, shp, dbf) 
{
  n <- nrow(data)
  polygons <- as.list(rep(NA, n))
  names(polygons) <- rownames(data)
  for (i in 1:n) {
    index <- which(dbf$dbf[, 1] == names(polygons)[i])
    if (length(index) == 0) {
    }
    else if (length(index) == 1) {
      shapefile <- shp$shp[[index]]
      if (shapefile$num.parts == 1) {
        temp <- Polygon(shapefile$points[,1:2])
        polygons[[i]] <- Polygons(list(temp), names(polygons)[i])
      }
      else {
        n.parts <- shapefile$num.parts
        results.part <- as.list(rep(0, n.parts))
        breakpoints <- c(shapefile$parts, shapefile$num.points)
        for (k in 1:n.parts) {
          start <- breakpoints[k] + 1
          end <- breakpoints[(k + 1)]
          results.part[[k]] <- Polygon(shapefile$points[start:end, 
                                                        1:2])
        }
        polygons[[i]] <- Polygons(results.part, names(polygons)[i])
      }
    }
    else {
      n.parts <- length(index)
      results.part <- as.list(rep(0, n.parts))
      for (k in 1:n.parts) {
        shapefile <- shp$shp[[index[k]]]
        results.part[k] <- Polygon(shapefile$points[,1:2])
      }
      polygons[[i]] <- Polygons(results.part, names(polygons)[i])
    }
  }
  na.check <- rep(0, n)
  for (i in 1:n) {
    if (class(polygons[[i]]) == "logical") 
      na.check[i] <- 1
  }
  if (sum(na.check) == n) 
    stop("None of the rownames of the data object match the first column of the dbf object.", 
         call. = FALSE)
  dataextend <- data.frame(rep(0, n), data)
  datatrimmed <- dataextend[na.check == 0, ]
  datatrimmed2 <- data.frame(datatrimmed[, -1])
  rownames(datatrimmed2) <- rownames(datatrimmed)
  colnames(datatrimmed2) <- colnames(data)
  polygonstrimmed <- polygons[na.check == 0]
  poly <- SpatialPolygons(polygonstrimmed)
  combined.data <- SpatialPolygonsDataFrame(poly, datatrimmed2)
  return(combined.data)
}
```

## Data import and clean

The data import and clean file are long and not the focus of this example. As a result, they were including in a seperate R file avaialble [here](https://github.com/MarkWang90/Tool-box/blob/master/wayfair_data/makedata/DataGeneration.r). If you download the entire folder [here](https://github.com/MarkWang90/Tool-box/tree/master/wayfair_data/makedata) and run the R file "DataGeneration.r", you should be able to generate the csv file "Paneldata.csv" which will be used in the rest of the project. 

```{r}
#### 2. Import Data
allPanel = read.csv("Paneldata.csv", header=TRUE);allPanel=allPanel[,-1]

shapefile <- readOGR('oneyear.shp')
coordinatess <- coordinates(shapefile)
shapefile.knn <- knearneigh(coordinatess, k = 4)
shapefile.nb <- knn2nb(shapefile.knn)
listw <- nb2listw(shapefile.nb)

shp_c<-read.shp(shp.name="cb_2014_us_county_500k.shp")
dbf_c<-read.dbf(dbf.name="cb_2014_us_county_500k.dbf")
dbf_c$dbf <-dbf_c$dbf[,c(5,1:4,6:9)]
temp=dbf_c$dbf[,1]
dbf_c$dbf$GEOID <- as.numeric(as.character(temp))

print("... Import data done ...")
```

## Data pre-analysis and plot
Next we map out and check the data. The four plots shows the four variables in our study region in the [Corn Belt](https://en.wikipedia.org/wiki/Corn_Belt) regions:
1. log corn yield
2. precipitation in cm during the growing season
3. cumulative [growing degress days](https://en.wikipedia.org/wiki/Growing_degree-day) above 0 celsius
4. cumulative growing degress days above 29 celsius

The data covers **782** counties from **1950 - 2015**. We only plot the county-level average for two time periods, namely 1975-1985, and 2005-2015 for illustration.

We can see from the plots that corn yield is highest in the Corn Belt, especially in Illinois and Iowa. We also see that precipitation generally decreases from east to west and the degree days increase from north to south. 

```{r}
#### 3. Data pre-analysis and plot
selectyears <- c(1985, 2015)
columnstoshow = c("fips","logcornyield","prec","lower","higher")
tenyear_average = data.frame()

for (year in selectyears){
  temp = allPanel[which(allPanel$year<=year &
                   allPanel$year>(year-10) ),columnstoshow] 
  temp$year = year
  
  tenyear_average = rbind(tenyear_average,aggregate(temp,list(temp$fips),mean))
}


tenyear_average=tenyear_average[,c(2,7,3,4,5,6)];head(tenyear_average)
names(tenyear_average)=c("fips","year","logcornyield","precip.","dday10c","dday29c")

MakeGraphs(tenyear_average[,c(1,2,3)],samescale = TRUE)
MakeGraphs(tenyear_average[,c(1,2,4)],samescale = TRUE)
MakeGraphs(tenyear_average[,c(1,2,5)],samescale = TRUE)
MakeGraphs(tenyear_average[,c(1,2,6)],samescale = TRUE)
```

## Model Fit
Now we examine the impact of climate change on corn yield, using log(cornyield) as dependent variable, and precipitation, growing degress days (at 0 and 29 celsius), as wel as squared terms and time trends as control variables. We fitted both **Conventional Panel Model** and **Spatial Panel Model** with fixed effects, considering the potential spatial correlation of corn yield (resulting from the similar soil, weather characteristics from nearby regions).

Also, we only used data until 2010 for training and hold out last five years for model testing.


```{r}
#### 4. Model fit  ####

## convert data into numeric values

data_train = allPanel[which(allPanel$year < 2011),]
formula <- logcornyield~lower+higher+prec+precsq+t+t2

## 4.1 Conventional Panel fixed effect model
fit1 <- plm(formula=formula, data = data_train,
            model = "within",
            effect = "individual")

## 4.2. Spatial panel model with fixed effect 
fit2 <- spml(formula=formula, data = data_train, 
             listw = listw, model = "within", 
             spatial.error = "b", Hess = FALSE)
```

## Model Performance Check

The hold-out data from 2011 to 2015 is used to calculate the out-of-sample MSE for both model. It seems that in this case, the Spatial Panel model did not show significant improvement as the two MSE are close.

```{r}
#### 5. Model performance
data_test <- allPanel[which(allPanel$year >= 2011),]
y_test <- data_test$logcornyield

## 5.1 PLM prediction
fixef <- rep(fixef(fit1), each=5)
coef <- fit1$coefficients
mod.mat <- model.matrix(logcornyield~lower+higher+prec+precsq+t+t2-1, 
                        data = data_test)
pred1 <- fixef + mod.mat%*%coef
mse_fit1 <- RMSE(y_test, pred1)
print(sprintf("out-of-sample mean squared error is %s for Conventional Panel", round(mse_fit1,3)))

## 5.2 SPML prediction

intercept <- rep(effects(fit2)$INTTable[,1], dim(mod.mat)[1])
fixef2 <- rep(effects(fit2)$SETable[,1], each=5)
coef2 <- fit2$coefficients[-1]

pred2 <- intercept + fixef2 + mod.mat%*%coef2
mse_fit2 <- RMSE(y_test, pred2)
print(sprintf("out-of-sample mean squared error is %s for Spatial Panel", round(mse_fit2,3)))

```

## Model tuning
One thing we can try to improve the performance of the Spatial Panel model is tuning a key parameter, namely the number of nearest neighbors (KNN) we choose to build the spatial weight matrix. We used KNN equals 4 in the previous estimation and would like to explore other options ranging from 1 to 7.

It seems that the optimal KNN would be 2 as showed in the plot below.

```{r}
#### 6. Model tuning
knn_list = seq(1,7,1)
mse_list = c()

for (knn in knn_list){
#  print(sprintf("setting # of nearest neighbour numbers to %s .", knn))
  shapefile.knn <- knearneigh(coordinatess, k = knn)
  shapefile.nb <- knn2nb(shapefile.knn)
  listw <- nb2listw(shapefile.nb)
  
  fit2 <- spml(formula=formula, data = data_train, 
             listw = listw, model = "within", 
             spatial.error = "b", Hess = FALSE)
  
  intercept <- rep(effects(fit2)$INTTable[,1], dim(mod.mat)[1])
  fixef2 <- rep(effects(fit2)$SETable[,1], each=5)
  coef2 <- fit2$coefficients[-1]

  pred2 <- intercept + fixef2 + mod.mat%*%coef2
#  print(RMSE(y_test, pred2))
  mse_list <- c(mse_list, RMSE(y_test, pred2))

}
plot(knn_list,mse_list,main="Model performance against # of K Nearest Neighbor",xlab="# of K nearest neighbour", ylab="Out-of-sample MSE")
```

## Model Interpretation
Finally we would like to address our research question: how would climate change affect corn productivity. Especially, we want to examine the impact of increasing temperature as heat is often regarded in the contraints in this region (for example see a study [here](http://igrow.org/agronomy/corn/drought-and-heat-effects-on-corn-production/). To do this, we generate the figure below, which shows the change in log yield when exposed to one additional day at a given temperature. We can see from the plot that moderate warm conditions would benefit corn productivity but once above 29 celsius, extreme heats would severely damage corn yield.

```{r}
#### Model Interpretation
fips <- 99900:99935
year <- rep(9999,length(fips))

added <-  data.frame(matrix(vector(), length(fips), dim(allPanel)[2]),
                         stringsAsFactors=F)
colnames(added)=colnames(allPanel) 

added$fips <- fips; added$year <- year
added$longitude <- mean(allPanel$longitude); added$latitude <- mean(allPanel$latitude)
added$t <- mean(allPanel$t); added$t2 <- mean(allPanel$t2)
added$prec <- mean(allPanel$prec); added$precsq <- mean(allPanel$precsq)

ddaysvalue <- c(0,5,8,10,12,15,20,25,29,30,31,32,33,34)
ddaycols <- colnames(allPanel)[which(grepl("dday",colnames(allPanel)) )]

names(ddaysvalue) <- ddaycols

meanvec <- apply(allPanel,2,mean)

count <- 0
for (i in which(grepl("dday",colnames(allPanel)) )){
  count <- count+1
  first <- which(added$fips-99900 == ddaysvalue[i-9+1])
  added[1:first,i] <- meanvec[i]
  added[first:length(fips),i] <- (added[first:length(fips),1]-99900-ddaysvalue[count])*2*pi+meanvec[i]
}

added$lower <- added$dday0C
added$higher <- added$dday29C

mod.mat.new <- model.matrix(~lower+higher+prec+precsq+t+t2-1, 
                        data = added)

pred1 <- rep(mean(fixef),length(fips)) + mod.mat.new%*%coef
pred2 <- intercept[1:length(fips)] + mod.mat.new%*%coef2

plot(0:35,pred1, ylim=c(4.20,4.55),type="l",xlab="Exposure to one additional day at certain degree - celsius", ylab="impact on log yield")
lines(0:35,pred2, ylim=c(4.20,4.55),col='red')
legend("bottomleft", legend=c("Conventional Panel", "Spatial Panel"),
       col=c("black", "red"), lty=1:2, cex=0.8)

```

